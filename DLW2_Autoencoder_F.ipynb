{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "CQqxPInwKxWl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5f442acd825c4f87878574dbbf99fd81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f18cfd51eb04d129da0b4788cd1548b",
              "IPY_MODEL_153d7acbdf47492da14f2a251b69a5fa",
              "IPY_MODEL_6c9df3cfa50942a2a2afb71b11d45ef1"
            ],
            "layout": "IPY_MODEL_8406bb43a1674e5aa1bd8227fdb32de1"
          }
        },
        "3f18cfd51eb04d129da0b4788cd1548b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a973e6dfdde4bb8a330f02ebe2f697a",
            "placeholder": "​",
            "style": "IPY_MODEL_a7e36c02f9a74adb916ec4f933183798",
            "value": "Epoch 14: 100%"
          }
        },
        "153d7acbdf47492da14f2a251b69a5fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c09ae922f764dabafc6086bce653114",
            "max": 594,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7150af05dcd445a972a284c0848fc99",
            "value": 594
          }
        },
        "6c9df3cfa50942a2a2afb71b11d45ef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_514b0f85b32c475d809a06c68be6271e",
            "placeholder": "​",
            "style": "IPY_MODEL_792c37f27af64e688d6e1ab57a904a84",
            "value": " 594/594 [06:38&lt;00:00,  1.49it/s, v_num=0, train_loss=5.260]"
          }
        },
        "8406bb43a1674e5aa1bd8227fdb32de1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "2a973e6dfdde4bb8a330f02ebe2f697a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7e36c02f9a74adb916ec4f933183798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c09ae922f764dabafc6086bce653114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7150af05dcd445a972a284c0848fc99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "514b0f85b32c475d809a06c68be6271e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "792c37f27af64e688d6e1ab57a904a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0bd95cf0b9b48b0ae62f10e869e8ae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f571408d2d74ccb9a9462fe54d16d70",
              "IPY_MODEL_9624bcf440d04ce2b0654596310e282a",
              "IPY_MODEL_93570097bf204baabee312c04eb1b1ca"
            ],
            "layout": "IPY_MODEL_7a8e2d90e3fa4e8eb4d7e8ffa0e5f58c"
          }
        },
        "2f571408d2d74ccb9a9462fe54d16d70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b52a28f8fc2847dabf4ee8b44541224b",
            "placeholder": "​",
            "style": "IPY_MODEL_bd7b56a1d4cd4d36bf7a4fd81bbb8da5",
            "value": "Epoch 0: 100%"
          }
        },
        "9624bcf440d04ce2b0654596310e282a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b434e43a0d394a9f85f03070b10cfc84",
            "max": 594,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a7f1d65b3b14dec92b9d9abdc3bf12f",
            "value": 594
          }
        },
        "93570097bf204baabee312c04eb1b1ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e088143188046f8b8b3800ee43d2dc0",
            "placeholder": "​",
            "style": "IPY_MODEL_80ec9735e8c04ba2a989e2d249b78d2c",
            "value": " 594/594 [05:27&lt;00:00,  1.82it/s, v_num=10, train_loss=2.760, train_accuracy=0.103]"
          }
        },
        "7a8e2d90e3fa4e8eb4d7e8ffa0e5f58c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "b52a28f8fc2847dabf4ee8b44541224b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd7b56a1d4cd4d36bf7a4fd81bbb8da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b434e43a0d394a9f85f03070b10cfc84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a7f1d65b3b14dec92b9d9abdc3bf12f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e088143188046f8b8b3800ee43d2dc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80ec9735e8c04ba2a989e2d249b78d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnvC1Tu-T-QI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a4d88f4-59cb-479f-8777-50f0ed93c053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting lightning\n",
            "  Downloading lightning-2.2.0.post0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2023.6.0)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.25.2)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (23.2)\n",
            "Requirement already satisfied: torch<4.0,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.1.0+cu121)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.4/840.4 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.9.0)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.2.0.post0-py3-none-any.whl (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.9/800.9 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (2.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.2.0.post0 lightning-utilities-0.10.1 pytorch-lightning-2.2.0.post0 torchmetrics-1.3.1\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install lightning\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "import lightning as L\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import Dataset, DataLoader,random_split,Subset, SubsetRandomSampler\n",
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import datasets\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint, EarlyStopping, Callback\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "id": "8Cag3oPYUDjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c bgu-i-know-what-you-did-last-measurement-time\n",
        "\n",
        "# Unzip the downloaded files and then remove the zip files\n",
        "!unzip bgu-i-know-what-you-did-last-measurement-time.zip"
      ],
      "metadata": {
        "id": "w5o4DanqUF75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlabeled_dir = '/content/unlabeled/unlabeled'\n",
        "\n",
        "unlabeled_files = os.listdir(unlabeled_dir)"
      ],
      "metadata": {
        "id": "CAddy1VzUIFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('train.csv')"
      ],
      "metadata": {
        "id": "67RfgoXEUTdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the label encoder and transform the 'activity' column to get encoded labels\n",
        "train_data['activity_encoded'] = label_encoder.fit_transform(train_data['activity'])"
      ],
      "metadata": {
        "id": "DPNJ-IR1UYqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzTNqr3zmisN"
      },
      "source": [
        "## **Part 2 - Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hShWcISloKqB"
      },
      "source": [
        "**a. validation strategy - train_test_split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvRD3W-umr-u",
        "outputId": "28020cae-ec44-459c-b2b6-5356e96c0b1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set includes 38001 samples from 6 subjects.\n",
            "Validation set includes 12247 samples from 2 subjects.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Identify unique users\n",
        "unique_subjects = train_data['userid'].unique()\n",
        "\n",
        "# Split user IDs into training and validation sets\n",
        "train_subjects, val_subjects = train_test_split(unique_subjects, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create training and validation sets\n",
        "train_set = train_data[train_data['userid'].isin(train_subjects)]\n",
        "val_set = train_data[train_data['userid'].isin(val_subjects)]\n",
        "\n",
        "# Summary of the split\n",
        "print(f\"Training set includes {len(train_set)} samples from {len(train_subjects)} subjects.\")\n",
        "print(f\"Validation set includes {len(val_set)} samples from {len(val_subjects)} subjects.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKj9aUSqo2RP"
      },
      "source": [
        "**c. Classical ML model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Odf_MuhUo153"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "def extract_statistical_features(df, data_dir):\n",
        "    \"\"\"\n",
        "    Extracts statistical features from time-series data corresponding to each row in the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame with at least an 'id' column to locate the data files.\n",
        "    - data_dir: The directory where the sequence data files are stored.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with original data augmented with statistical features.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    means, stds, mins, maxs, medians, skews, kurtoses = [], [], [], [], [], [], []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        seq_id = row['id']\n",
        "        file_path = os.path.join(data_dir, f\"{seq_id}.csv\")\n",
        "        seq_df = pd.read_csv(file_path)\n",
        "\n",
        "        # Filter by measurement type if necessary\n",
        "        if \"measurement type\" in seq_df.columns:\n",
        "            seq_df = seq_df[seq_df[\"measurement type\"] == \"acceleration [m/s/s]\"]\n",
        "\n",
        "        # Calculate statistical features for each axis\n",
        "        for axis in ['x', 'y', 'z']:\n",
        "            axis_data = seq_df[axis] if axis in seq_df.columns else seq_df[f\"{axis} [m]\"]\n",
        "            means.append(axis_data.mean())\n",
        "            stds.append(axis_data.std())\n",
        "            mins.append(axis_data.min())\n",
        "            maxs.append(axis_data.max())\n",
        "            medians.append(axis_data.median())\n",
        "            skews.append(skew(axis_data))  # Corrected\n",
        "            kurtoses.append(kurtosis(axis_data))  # Corrected\n",
        "\n",
        "    # Create a DataFrame with the calculated features\n",
        "    features_df = pd.DataFrame({\n",
        "        'x_mean': means[0::3], 'y_mean': means[1::3], 'z_mean': means[2::3],\n",
        "        'x_std': stds[0::3], 'y_std': stds[1::3], 'z_std': stds[2::3],\n",
        "        'x_min': mins[0::3], 'y_min': mins[1::3], 'z_min': mins[2::3],\n",
        "        'x_max': maxs[0::3], 'y_max': maxs[1::3], 'z_max': maxs[2::3],\n",
        "        'x_median': medians[0::3], 'y_median': medians[1::3], 'z_median': medians[2::3],\n",
        "        'x_skew': skews[0::3], 'y_skew': skews[1::3], 'z_skew': skews[2::3],\n",
        "        'x_kurtosis': kurtoses[0::3], 'y_kurtosis': kurtoses[1::3], 'z_kurtosis': kurtoses[2::3],\n",
        "    })\n",
        "\n",
        "    # Return the original DataFrame augmented with the new features\n",
        "    return pd.concat([df.reset_index(drop=True), features_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the train_features DataFrame from the saved CSV file\n",
        "train_features_loaded = pd.read_csv('/content/train_features_1.csv')\n",
        "\n",
        "# Load the val_features DataFrame from the saved CSV file\n",
        "val_features_loaded = pd.read_csv('/content/val_features_1.csv')"
      ],
      "metadata": {
        "id": "wAiJNPQ5FQYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_columns = ['x_mean', 'y_mean', 'z_mean', 'x_std', 'y_std', 'z_std','x_min', 'y_min', 'z_min','x_max', 'y_max', 'z_max','x_median', 'y_median', 'z_median', 'x_skew', 'y_skew', 'z_skew','x_kurtosis', 'y_kurtosis', 'z_kurtosis']"
      ],
      "metadata": {
        "id": "POLi-LFfYvi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'features_columns' is a list of your feature column names\n",
        "X_train_loaded = train_features_loaded[features_columns].values\n",
        "X_val_loaded = val_features_loaded[features_columns].values\n",
        "\n",
        "y_train_loaded = train_features_loaded['activity_encoded'].values\n",
        "y_val_loaded = val_features_loaded['activity_encoded'].values"
      ],
      "metadata": {
        "id": "lvSaCXa9innS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08QgJfohvovZ"
      },
      "outputs": [],
      "source": [
        "class HARDataset(Dataset):\n",
        "    def __init__(self, data, data_dir, transform=None, labeled=True):\n",
        "        \"\"\"\n",
        "        Initializes the dataset.\n",
        "        :param data: DataFrame for labeled data or list of filenames for unlabeled data.\n",
        "        :param data_dir: Base directory where the data files are stored.\n",
        "        :param transform: Optional transform to be applied on a sample.\n",
        "        :param labeled: Flag indicating if the dataset is for labeled or unlabeled data.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.labeled = labeled\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.labeled:\n",
        "            # Handling labeled data\n",
        "            seq_id, activity_label = self.data.iloc[idx]['id'], self.data.iloc[idx]['activity_encoded']\n",
        "            seq_file_path = os.path.join(self.data_dir, f\"{seq_id}.csv\")\n",
        "        else:# Handling unlabeled data\n",
        "            seq_file_path = os.path.join(self.data_dir, self.data[idx])\n",
        "\n",
        "        # Load sequence data\n",
        "        seq_frame = pd.read_csv(seq_file_path)\n",
        "        if self.labeled and \"measurement type\" in seq_frame.columns:\n",
        "            # Filter by acceleration if it's labeled data and contains a 'measurement type' column\n",
        "            seq_frame = seq_frame[seq_frame[\"measurement type\"] == \"acceleration [m/s/s]\"]\n",
        "            sequence_data = seq_frame.iloc[:, 1:4].values\n",
        "        else:\n",
        "            sequence_data = seq_frame.values  # Use all data if no filtering criteria\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            sequence_data = self.transform(sequence_data)\n",
        "        sequence_data_tensor = torch.tensor(sequence_data, dtype=torch.float32)\n",
        "\n",
        "        if self.labeled:\n",
        "            activity_label = self.data.iloc[idx]['activity_encoded']\n",
        "            activity_label_tensor = torch.tensor(activity_label, dtype=torch.long)\n",
        "            return sequence_data_tensor, activity_label_tensor\n",
        "        else:\n",
        "            return sequence_data_tensor  # No label for unlabeled data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Padding from start for LSTM\n",
        "\n",
        "def pad_sequences_from_start(batch, max_length=3000):\n",
        "    # Initialize padded sequences and labels\n",
        "    # Assuming each sequence has the same number of features\n",
        "    num_features = batch[0][0].shape[1]\n",
        "    padded_sequences = torch.zeros(len(batch), max_length, num_features)\n",
        "    # labels = []\n",
        "    labels = torch.zeros(len(batch), dtype=torch.long)\n",
        "\n",
        "    for i, (sequence, label) in enumerate(batch):\n",
        "        length = sequence.shape[0]\n",
        "        start = max(max_length - length, 0)  # Calculate start index for sequence\n",
        "        if length > max_length:\n",
        "            # If sequence is longer than max_length, truncate it from the end\n",
        "            padded_sequence = sequence[-max_length:]\n",
        "        else:\n",
        "            padded_sequences[i, start:] = sequence\n",
        "\n",
        "        labels[i] = label\n",
        "\n",
        "    return padded_sequences, labels"
      ],
      "metadata": {
        "id": "l1c5l2rMU4Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequences_from_start(batch, max_length=3000):\n",
        "    # Initialize padded sequences and labels\n",
        "    num_features = batch[0].shape[1] if type(batch[0]) == torch.Tensor else batch[0][0].shape[1]\n",
        "    padded_sequences = torch.zeros(len(batch), max_length, num_features)\n",
        "    labels = None  # Initialize labels as None\n",
        "\n",
        "    for i, data in enumerate(batch):\n",
        "        if isinstance(data, tuple):  # Check if data has labels\n",
        "            sequence, label = data\n",
        "            if labels is None:  # Initialize labels if they exist\n",
        "                labels = torch.zeros(len(batch), dtype=torch.long)\n",
        "            labels[i] = label\n",
        "        else:\n",
        "            sequence = data  # No label present\n",
        "\n",
        "        length = sequence.shape[0]\n",
        "        start = max(max_length - length, 0)  # Calculate start index for sequence\n",
        "        if length > max_length:\n",
        "            padded_sequence = sequence[-max_length:]\n",
        "        else:\n",
        "            padded_sequences[i, start:] = sequence\n",
        "\n",
        "    return (padded_sequences, labels) if labels is not None else padded_sequences\n"
      ],
      "metadata": {
        "id": "8aSVQpkekeM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0vultSivw-G"
      },
      "outputs": [],
      "source": [
        "# Create dataset instances\n",
        "train_dataset = HARDataset(train_features_loaded, unlabeled_dir, labeled=True)\n",
        "val_dataset = HARDataset(val_features_loaded, unlabeled_dir, labeled=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kJVUCmcvypj"
      },
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=pad_sequences_from_start)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=pad_sequences_from_start)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (x, y) in enumerate(train_dataloader):\n",
        "    if i == 5:\n",
        "        break\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    print(f\"X shape: {x.shape}\")\n",
        "    print(f\"y shape: {y.shape}\")\n",
        "    print(f\"y: {y}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "g9zdQqGvjbtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Autoencoder**"
      ],
      "metadata": {
        "id": "4R6UmLkIBBzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **#Unlabeled**"
      ],
      "metadata": {
        "id": "CQqxPInwKxWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "def extract_statistical_features(data_dir, file_list):\n",
        "    features = []\n",
        "\n",
        "    for file_name in file_list:\n",
        "        file_path = os.path.join(data_dir, file_name)\n",
        "        seq_df = pd.read_csv(file_path)\n",
        "\n",
        "        if \"measurement type\" in seq_df.columns:\n",
        "            seq_df_filtered = seq_df[seq_df[\"measurement type\"].str.contains(\"acceleration\")].copy()\n",
        "            seq_df_filtered.drop(columns=['measurement type'], inplace=True)\n",
        "        else:\n",
        "            seq_df_filtered = seq_df.copy()\n",
        "\n",
        "        if 'x [m]' in seq_df_filtered.columns:\n",
        "            seq_df_filtered.rename(columns={'x [m]': 'x', 'y [m]': 'y', 'z [m]': 'z'}, inplace=True)\n",
        "\n",
        "        stats = []\n",
        "        for axis in ['x', 'y', 'z']:\n",
        "            axis_data = seq_df_filtered[axis]\n",
        "            stats.extend([axis_data.mean(), axis_data.std()])\n",
        "\n",
        "        features.append(stats)\n",
        "\n",
        "    columns = ['x_mean', 'y_mean', 'z_mean', 'x_std', 'y_std', 'z_std']\n",
        "\n",
        "    features_df = pd.DataFrame(features, columns=columns)\n",
        "    return features_df\n"
      ],
      "metadata": {
        "id": "fNcA83SCDVlR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequences_from_start_unlabeled(batch, max_length=4000):\n",
        "    num_features = batch[0].shape[1]  # Adjusted for sequences only, no labels\n",
        "    padded_sequences = torch.zeros(len(batch), max_length, num_features)\n",
        "\n",
        "    for i, sequence in enumerate(batch):  # No labels to unpack\n",
        "        length = sequence.shape[0]\n",
        "        if length > max_length:\n",
        "            padded_sequences[i] = sequence[-max_length:]  # Truncate from the start\n",
        "        else:\n",
        "            start = max_length - length  # Calculate start index for sequence\n",
        "            padded_sequences[i, start:] = sequence  # Pad at the beginning\n",
        "\n",
        "    return padded_sequences"
      ],
      "metadata": {
        "id": "TTV5U88OIPvw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnlabeledSequenceDataset(Dataset):\n",
        "    def __init__(self, sequence_dir, features_df, file_list):\n",
        "        self.sequence_dir = sequence_dir\n",
        "        self.features_df = features_df  # DataFrame of features\n",
        "        self.file_list = file_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_name = self.file_list[idx].split('.')[0]  # Assuming file_list contains file names with '.csv' extension\n",
        "        seq_file_path = os.path.join(self.sequence_dir, self.file_list[idx])\n",
        "        sequence = pd.read_csv(seq_file_path)\n",
        "\n",
        "        if 'measurement type' in sequence.columns:\n",
        "            sequence_filtered = sequence[sequence['measurement type'] == 'acceleration [m/s/s]'].copy()\n",
        "            sequence_filtered.drop(columns=['measurement type'], inplace=True)\n",
        "            sequence_filtered.rename(columns=lambda x: x.strip().split(' ')[0], inplace=True)\n",
        "        else:\n",
        "            sequence_filtered = sequence.copy()\n",
        "\n",
        "        # Check and convert axis names if they are in 'x [m]', 'y [m]', 'z [m]' format\n",
        "        if set(['x [m]', 'y [m]', 'z [m]']).issubset(sequence_filtered.columns):\n",
        "            sequence_filtered.rename(columns={'x [m]': 'x', 'y [m]': 'y', 'z [m]': 'z'}, inplace=True)\n",
        "\n",
        "        sequence_tensor = torch.tensor(sequence_filtered[['x', 'y', 'z']].values, dtype=torch.float32)\n",
        "\n",
        "        features = self.features_df.loc[self.features_df.index == file_name]\n",
        "        if features.empty:\n",
        "            features_tensor = torch.zeros(self.features_df.shape[1])\n",
        "        else:\n",
        "            features_tensor = torch.tensor(features.values[0], dtype=torch.float32)  # Assuming features are in the first row if present\n",
        "\n",
        "        return sequence_tensor, features_tensor"
      ],
      "metadata": {
        "id": "hb0Yi4yrrQhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "\n",
        "# # Example usage with a subset of data\n",
        "# data_dir = unlabeled_dir  # Directory where the sequence data files are stored\n",
        "# all_file_list = os.listdir(data_dir)  # Get all file names in the directory\n",
        "\n",
        "# # Select a random 5% of the files\n",
        "# sample_size = int(len(all_file_list) * 0.05)  # Calculate 5% of the total number of files\n",
        "# sampled_file_list = random.sample(all_file_list, sample_size)  # Randomly select files\n"
      ],
      "metadata": {
        "id": "N6CwdkS3aCXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, use only this subset for your feature extraction\n",
        "# unlabeled_features_sample = extract_statistical_features(data_dir, sampled_file_list)"
      ],
      "metadata": {
        "id": "FjjaJmNcaLEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure this list corresponds to the actual files used to create 'unlabeled_features_sample'\n",
        "# sampled_files = random.sample(unlabeled_files, len(unlabeled_features_sample))  # Or however you've obtained the sample\n",
        "# unlabeled_features_sample.index = [name.split('.')[0] for name in sampled_file_list]\n"
      ],
      "metadata": {
        "id": "46SkfWdWjO4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    # Extract sequences and features from the batch\n",
        "    sequences = [item[0] for item in batch]  # Extract sequences\n",
        "    features = [item[1] for item in batch]  # Extract features\n",
        "\n",
        "    # Pad the sequences\n",
        "    max_length = 4000  # Adjust as necessary\n",
        "    padded_sequences = pad_sequences_from_start_unlabeled(sequences, max_length)\n",
        "\n",
        "    # Convert features to a tensor (since they should all be the same length already)\n",
        "    features_tensor = torch.stack(features)\n",
        "\n",
        "    return padded_sequences, features_tensor"
      ],
      "metadata": {
        "id": "Q58Q89Y0pXIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create your dataset using the sampled files and their corresponding features\n",
        "# unlabeled_dataset_sample = UnlabeledSequenceDataset(data_dir, unlabeled_features_sample, sampled_file_list)\n",
        "\n",
        "# # Now, create your DataLoader from this dataset\n",
        "# unlabeled_dataloader_sample = DataLoader(unlabeled_dataset_sample, batch_size=64, shuffle=True, collate_fn=custom_collate_fn)\n"
      ],
      "metadata": {
        "id": "aNgMMJkAj5HA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, (sequences, features) in enumerate(unlabeled_dataloader_sample):\n",
        "#     # sequences: padded sequence tensors\n",
        "#     # features: corresponding statistical feature tensors\n",
        "#     # Add your training code here\n",
        "#     print(f\"Batch {i + 1}: Sequences shape = {sequences.shape}, Features shape = {features.shape}\")\n",
        "#     if i == 5:  # Just an example to limit the loop for testing\n",
        "#         break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDaG7lZvkT47",
        "outputId": "7a2dcfbc-0568-425d-b2dd-7b5a8c759dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: Sequences shape = torch.Size([64, 4000, 3]), Features shape = torch.Size([64, 21])\n",
            "Batch 2: Sequences shape = torch.Size([64, 4000, 3]), Features shape = torch.Size([64, 21])\n",
            "Batch 3: Sequences shape = torch.Size([64, 4000, 3]), Features shape = torch.Size([64, 21])\n",
            "Batch 4: Sequences shape = torch.Size([64, 4000, 3]), Features shape = torch.Size([64, 21])\n",
            "Batch 5: Sequences shape = torch.Size([64, 4000, 3]), Features shape = torch.Size([64, 21])\n",
            "Batch 6: Sequences shape = torch.Size([64, 4000, 3]), Features shape = torch.Size([64, 21])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features\n",
        "unlabeled_features = extract_statistical_features(unlabeled_dir, unlabeled_files)"
      ],
      "metadata": {
        "id": "Lk3vRIDUvNIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlabeled_features.index = [name.split('.')[0] for name in unlabeled_files]\n",
        "# unlabeled_features.index = [name.replace('.csv', '') for name in unlabeled_files]  # Set file names (without '.csv') as index"
      ],
      "metadata": {
        "id": "X6Fsr4erzW0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming unlabeled_features is a pandas DataFrame\n",
        "unlabeled_features.to_csv('unlabeled_features.csv', index=False)"
      ],
      "metadata": {
        "id": "iJ6O4JiuxN4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the dataset\n",
        "unlabeled_dataset = UnlabeledSequenceDataset(\n",
        "    sequence_dir=unlabeled_dir,\n",
        "    features_df=unlabeled_features,\n",
        "    file_list=unlabeled_files\n",
        ")\n",
        "\n",
        "# Create the DataLoader\n",
        "unlabeled_dataloader = DataLoader(\n",
        "    dataset=unlabeled_dataset,\n",
        "    batch_size=64,  # You can adjust the batch size according to your needs\n",
        "    shuffle=True,   # Shuffle the data to ensure random sampling\n",
        "    collate_fn=custom_collate_fn,  # Use your custom collate function\n",
        "    drop_last=True  # Drop the last batch if it's smaller than the specified batch size\n",
        ")"
      ],
      "metadata": {
        "id": "xBn6p-OTVpGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After extracting features, ensure the index matches the modified file names exactly.\n",
        "unlabeled_features.index = [name.split('.')[0] for name in unlabeled_files]  # Assuming the file names are like '1641.csv'\n"
      ],
      "metadata": {
        "id": "hCp09Zcfh-9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (sequences, features) in enumerate(unlabeled_dataloader):\n",
        "    # sequences: padded sequence tensors\n",
        "    # features: corresponding statistical feature tensors\n",
        "    # Add your training code here\n",
        "    print(f\"Batch {i + 1}: Sequences shape = {sequences.shape}, Features shape = {features.shape}\")\n",
        "    if i == 5:  # Just an example to limit the loop for testing\n",
        "        break"
      ],
      "metadata": {
        "id": "F53IS753Vus1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e1fa10-276c-47c7-9a56-24bd89e42bd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: Sequences shape = torch.Size([64, 4000, 3]), Features shape = torch.Size([64, 6])\n",
            "Batch 2: Sequences shape = torch.Size([64, 4000, 3]), Features shape = torch.Size([64, 6])\n",
            "Batch 3: Sequences shape = torch.Size([64, 4000, 3]), Features shape = torch.Size([64, 6])\n",
            "Batch 4: Sequences shape = torch.Size([64, 4000, 3]), Features shape = torch.Size([64, 6])\n",
            "Batch 5: Sequences shape = torch.Size([64, 4000, 3]), Features shape = torch.Size([64, 6])\n",
            "Batch 6: Sequences shape = torch.Size([64, 4000, 3]), Features shape = torch.Size([64, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **#Define the Autoencoder for Pretraining**"
      ],
      "metadata": {
        "id": "HoWsRtkTBMWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#Try**"
      ],
      "metadata": {
        "id": "NVqBL-IJrspn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ZsZdED01siv0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "def extract_statistical_features(df, data_dir):\n",
        "    \"\"\"\n",
        "    Extracts statistical features from time-series data corresponding to each row in the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame with at least an 'id' column to locate the data files.\n",
        "    - data_dir: The directory where the sequence data files are stored.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with original data augmented with statistical features.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    means, stds, mins, maxs, medians, skews, kurtoses = [], [], [], [], [], [], []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        seq_id = row['id']\n",
        "        file_path = os.path.join(data_dir, f\"{seq_id}.csv\")\n",
        "        seq_df = pd.read_csv(file_path)\n",
        "\n",
        "        # Filter by measurement type if necessary\n",
        "        if \"measurement type\" in seq_df.columns:\n",
        "            seq_df = seq_df[seq_df[\"measurement type\"] == \"acceleration [m/s/s]\"]\n",
        "\n",
        "        # Calculate statistical features for each axis\n",
        "        for axis in ['x', 'y', 'z']:\n",
        "            axis_data = seq_df[axis] if axis in seq_df.columns else seq_df[f\"{axis} [m]\"]\n",
        "            means.append(axis_data.mean())\n",
        "            stds.append(axis_data.std())\n",
        "            mins.append(axis_data.min())\n",
        "            maxs.append(axis_data.max())\n",
        "            medians.append(axis_data.median())\n",
        "            skews.append(skew(axis_data))  # Corrected\n",
        "            kurtoses.append(kurtosis(axis_data))  # Corrected\n",
        "\n",
        "    # Create a DataFrame with the calculated features\n",
        "    features_df = pd.DataFrame({\n",
        "        'x_mean': means[0::3], 'y_mean': means[1::3], 'z_mean': means[2::3],\n",
        "        'x_std': stds[0::3], 'y_std': stds[1::3], 'z_std': stds[2::3],\n",
        "        'x_min': mins[0::3], 'y_min': mins[1::3], 'z_min': mins[2::3],\n",
        "        'x_max': maxs[0::3], 'y_max': maxs[1::3], 'z_max': maxs[2::3],\n",
        "        'x_median': medians[0::3], 'y_median': medians[1::3], 'z_median': medians[2::3],\n",
        "        'x_skew': skews[0::3], 'y_skew': skews[1::3], 'z_skew': skews[2::3],\n",
        "        'x_kurtosis': kurtoses[0::3], 'y_kurtosis': kurtoses[1::3], 'z_kurtosis': kurtoses[2::3],\n",
        "    })\n",
        "\n",
        "    # Return the original DataFrame augmented with the new features\n",
        "    return pd.concat([df.reset_index(drop=True), features_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the train_features DataFrame from the saved CSV file\n",
        "train_features_loaded = pd.read_csv('/content/train_features_1.csv')\n",
        "\n",
        "# Load the val_features DataFrame from the saved CSV file\n",
        "val_features_loaded = pd.read_csv('/content/val_features_1.csv')"
      ],
      "metadata": {
        "id": "TNYCFLWnsiv1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_columns = ['x_mean', 'y_mean', 'z_mean', 'x_std', 'y_std', 'z_std','x_min', 'y_min', 'z_min','x_max', 'y_max', 'z_max','x_median', 'y_median', 'z_median', 'x_skew', 'y_skew', 'z_skew','x_kurtosis', 'y_kurtosis', 'z_kurtosis']"
      ],
      "metadata": {
        "id": "pOQb4dxqsiv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir, file_list, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (string): Directory with all the sequences.\n",
        "            file_list (DataFrame): DataFrame containing file names and labels.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.data = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if isinstance(self.data, pd.DataFrame):\n",
        "            seq_id = self.data.iloc[idx]['id']  # Assuming 'id' contains file names without '.csv' extension\n",
        "            seq_file_path = os.path.join(self.data_dir, f\"{seq_id}.csv\")\n",
        "        else:\n",
        "            # Handle the case where self.data is not a DataFrame but a list or similar\n",
        "            seq_file_path = os.path.join(self.data_dir, self.data[idx])\n",
        "\n",
        "        # Load sequence\n",
        "        sequence = pd.read_csv(seq_file_path)\n",
        "\n",
        "        # Preprocess and filter sequence data if necessary\n",
        "        if 'measurement type' in sequence.columns:\n",
        "            sequence_filtered = sequence[sequence['measurement type'] == 'acceleration [m/s/s]'].copy()\n",
        "            sequence_filtered.drop(columns=['measurement type'], inplace=True)\n",
        "            sequence_filtered.rename(columns=lambda x: x.strip().split(' ')[0], inplace=True)\n",
        "        else:\n",
        "            sequence_filtered = sequence.copy()\n",
        "\n",
        "        # Convert axis names if in specific format\n",
        "        if set(['x [m]', 'y [m]', 'z [m]']).issubset(sequence_filtered.columns):\n",
        "            sequence_filtered.rename(columns={'x [m]': 'x', 'y [m]': 'y', 'z [m]': 'z'}, inplace=True)\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            sequence_filtered = self.transform(sequence_filtered)  # Ensure your transform can handle a DataFrame or a numpy array\n",
        "\n",
        "        # Convert filtered sequence to tensor\n",
        "        sequence_data_tensor = torch.tensor(sequence_filtered[['x', 'y', 'z']].values, dtype=torch.float32)\n",
        "\n",
        "        # For autoencoder, return the same data as both input and output\n",
        "        return sequence_data_tensor, sequence_data_tensor\n",
        "\n"
      ],
      "metadata": {
        "id": "9aZpGvyyrr4V"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequences_from_start(batch, max_length=4000):\n",
        "    # Initialize padded sequences\n",
        "    num_features = batch[0].shape[1]  # Adjusted for batch without labels\n",
        "    padded_sequences = torch.zeros(len(batch), max_length, num_features)\n",
        "\n",
        "    for i, sequence in enumerate(batch):\n",
        "        length = sequence.shape[0]\n",
        "        start = max(max_length - length, 0)  # Calculate start index for sequence\n",
        "        if length > max_length:\n",
        "            # If sequence is longer than max_length, truncate it from the end\n",
        "            padded_sequence = sequence[-max_length:]\n",
        "        else:\n",
        "            padded_sequences[i, start:] = sequence\n",
        "\n",
        "    return padded_sequences  # Return only the padded sequences for unlabeled data"
      ],
      "metadata": {
        "id": "zDzFztAQr7-q"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset instances\n",
        "train_dataset = CustomDataset(train_features_loaded, unlabeled_dir, labeled=False)\n",
        "val_dataset = CustomDataset(val_features_loaded, unlabeled_dir, labeled=False)\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=pad_sequences_from_start)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True, collate_fn=pad_sequences_from_start)"
      ],
      "metadata": {
        "id": "dTg436t4sBJt"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (sequences, targets) in enumerate(train_dataloader):\n",
        "    print(f\"Batch {i + 1}:\")\n",
        "    print(f\"Sequences shape: {sequences.shape}\")\n",
        "    print(f\"Targets shape: {targets.shape}\")  # If you're using targets, otherwise ignore\n",
        "    if i == 2:  # Check first three batches\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "vL-iJ37_1DHE",
        "outputId": "b9dc7366-d285-4639-aea4-ec4d0a2015cc"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "3675",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 3675",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-420258a7c46d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch {i + 1}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Sequences shape: {sequences.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Targets shape: {targets.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# If you're using targets, otherwise ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Check first three batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-ed4b5db3d07c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mseq_id\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# Assuming file_list contains file names with '.csv' extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mseq_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 3675"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the configuration for train_dataloader\n",
        "print(\"Checking configuration for train_dataloader:\")\n",
        "for i, x in enumerate(train_dataloader):\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    print(f\"X shape: {x.shape}\")  # x is the batch of sequences\n",
        "    if i >= 2:  # Print the first 3 batches to check their shapes\n",
        "        break\n",
        "\n",
        "# Add a separator for clarity\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Check the configuration for val_dataloader\n",
        "print(\"Checking configuration for val_dataloader:\")\n",
        "for i, x in enumerate(val_dataloader):\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    print(f\"X shape: {x.shape}\")  # x is the batch of sequences\n",
        "    if i >= 2:  # Print the first 3 batches to check their shapes\n",
        "        break\n"
      ],
      "metadata": {
        "id": "CkZxbVXxsoib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "########################3"
      ],
      "metadata": {
        "id": "4lsW4kIa3HbH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ZT-Vks73O4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "45KslAcN3PzK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "def extract_statistical_features(df, data_dir):\n",
        "    \"\"\"\n",
        "    Extracts statistical features from time-series data corresponding to each row in the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame with at least an 'id' column to locate the data files.\n",
        "    - data_dir: The directory where the sequence data files are stored.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with original data augmented with statistical features.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    means, stds, mins, maxs, medians, skews, kurtoses = [], [], [], [], [], [], []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        seq_id = row['id']\n",
        "        file_path = os.path.join(data_dir, f\"{seq_id}.csv\")\n",
        "        seq_df = pd.read_csv(file_path)\n",
        "\n",
        "        # Filter by measurement type if necessary\n",
        "        if \"measurement type\" in seq_df.columns:\n",
        "            seq_df = seq_df[seq_df[\"measurement type\"] == \"acceleration [m/s/s]\"]\n",
        "\n",
        "        # Calculate statistical features for each axis\n",
        "        for axis in ['x', 'y', 'z']:\n",
        "            axis_data = seq_df[axis] if axis in seq_df.columns else seq_df[f\"{axis} [m]\"]\n",
        "            means.append(axis_data.mean())\n",
        "            stds.append(axis_data.std())\n",
        "            mins.append(axis_data.min())\n",
        "            maxs.append(axis_data.max())\n",
        "            medians.append(axis_data.median())\n",
        "            skews.append(skew(axis_data))  # Corrected\n",
        "            kurtoses.append(kurtosis(axis_data))  # Corrected\n",
        "\n",
        "    # Create a DataFrame with the calculated features\n",
        "    features_df = pd.DataFrame({\n",
        "        'x_mean': means[0::3], 'y_mean': means[1::3], 'z_mean': means[2::3],\n",
        "        'x_std': stds[0::3], 'y_std': stds[1::3], 'z_std': stds[2::3],\n",
        "        'x_min': mins[0::3], 'y_min': mins[1::3], 'z_min': mins[2::3],\n",
        "        'x_max': maxs[0::3], 'y_max': maxs[1::3], 'z_max': maxs[2::3],\n",
        "        'x_median': medians[0::3], 'y_median': medians[1::3], 'z_median': medians[2::3],\n",
        "        'x_skew': skews[0::3], 'y_skew': skews[1::3], 'z_skew': skews[2::3],\n",
        "        'x_kurtosis': kurtoses[0::3], 'y_kurtosis': kurtoses[1::3], 'z_kurtosis': kurtoses[2::3],\n",
        "    })\n",
        "\n",
        "    # Return the original DataFrame augmented with the new features\n",
        "    return pd.concat([df.reset_index(drop=True), features_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the train_features DataFrame from the saved CSV file\n",
        "train_features_loaded = pd.read_csv('/content/train_features_1.csv')\n",
        "\n",
        "# Load the val_features DataFrame from the saved CSV file\n",
        "val_features_loaded = pd.read_csv('/content/val_features_1.csv')"
      ],
      "metadata": {
        "id": "8rzuKmq83Pze"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_columns = ['x_mean', 'y_mean', 'z_mean', 'x_std', 'y_std', 'z_std','x_min', 'y_min', 'z_min','x_max', 'y_max', 'z_max','x_median', 'y_median', 'z_median', 'x_skew', 'y_skew', 'z_skew','x_kurtosis', 'y_kurtosis', 'z_kurtosis']"
      ],
      "metadata": {
        "id": "SpcbM6Th3Pzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "eGyebjpC3Pzf"
      },
      "outputs": [],
      "source": [
        "class HARDataset(Dataset):\n",
        "    def __init__(self, data, data_dir, transform=None, labeled=True):\n",
        "        \"\"\"\n",
        "        Initializes the dataset.\n",
        "        :param data: DataFrame for labeled data or list of filenames for unlabeled data.\n",
        "        :param data_dir: Base directory where the data files are stored.\n",
        "        :param transform: Optional transform to be applied on a sample.\n",
        "        :param labeled: Flag indicating if the dataset is for labeled or unlabeled data.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.labeled = labeled\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.labeled:\n",
        "            # Handling labeled data\n",
        "            seq_id, activity_label = self.data.iloc[idx]['id'], self.data.iloc[idx]['activity_encoded']\n",
        "            seq_file_path = os.path.join(self.data_dir, f\"{seq_id}.csv\")\n",
        "        else:# Handling unlabeled data\n",
        "            seq_file_path = os.path.join(self.data_dir, self.data[idx])\n",
        "\n",
        "        # Load sequence data\n",
        "        seq_frame = pd.read_csv(seq_file_path)\n",
        "        if  \"measurement type\" in seq_frame.columns:\n",
        "            # Filter by acceleration if it's labeled data and contains a 'measurement type' column\n",
        "            seq_frame = seq_frame[seq_frame[\"measurement type\"] == \"acceleration [m/s/s]\"]\n",
        "            sequence_data = seq_frame.iloc[:, 1:4].values\n",
        "        else:\n",
        "            sequence_data = seq_frame.values  # Use all data if no filtering criteria\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            sequence_data = self.transform(sequence_data)\n",
        "        sequence_data_tensor = torch.tensor(sequence_data, dtype=torch.float32)\n",
        "\n",
        "        if self.labeled:\n",
        "            activity_label = self.data.iloc[idx]['activity_encoded']\n",
        "            activity_label_tensor = torch.tensor(activity_label, dtype=torch.long)\n",
        "            return sequence_data_tensor, activity_label_tensor\n",
        "        else:\n",
        "            return sequence_data_tensor  # No label for unlabeled data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Padding from start for LSTM\n",
        "\n",
        "def pad_sequences_from_start(batch, max_length=3000):\n",
        "    # Initialize padded sequences and labels\n",
        "    # Assuming each sequence has the same number of features\n",
        "    num_features = batch[0][0].shape[1]\n",
        "    padded_sequences = torch.zeros(len(batch), max_length, num_features)\n",
        "    # labels = []\n",
        "    labels = torch.zeros(len(batch), dtype=torch.long)\n",
        "\n",
        "    for i, (sequence, label) in enumerate(batch):\n",
        "        length = sequence.shape[0]\n",
        "        start = max(max_length - length, 0)  # Calculate start index for sequence\n",
        "        if length > max_length:\n",
        "            # If sequence is longer than max_length, truncate it from the end\n",
        "            padded_sequence = sequence[-max_length:]\n",
        "        else:\n",
        "            padded_sequences[i, start:] = sequence\n",
        "\n",
        "        labels[i] = label\n",
        "\n",
        "    return padded_sequences, labels"
      ],
      "metadata": {
        "id": "pIBMR16H3Pzf"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "e9BlIILT3Pzg"
      },
      "outputs": [],
      "source": [
        "# Create dataset instances\n",
        "train_dataset = HARDataset(train_features_loaded, unlabeled_dir, labeled=True)\n",
        "val_dataset = HARDataset(val_features_loaded, unlabeled_dir, labeled=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "o3Q948603Pzg"
      },
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=pad_sequences_from_start)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True, collate_fn=pad_sequences_from_start)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (x,y) in enumerate(train_dataloader):\n",
        "    if i == 5:\n",
        "        break\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    print(f\"X shape: {x.shape}\")\n",
        "    print(f\"y shape: {y.shape}\")\n",
        "    print(f\"y: {y}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7fjqRF63eCP",
        "outputId": "dd9536ec-f8fa-4874-bd92-81b187fa47e3"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1:\n",
            "X shape: torch.Size([64, 3000, 3])\n",
            "y shape: torch.Size([64])\n",
            "y: tensor([ 7, 10,  5, 16,  6,  9, 10, 16,  9,  9, 13,  7,  8,  9,  9, 12,  9,  3,\n",
            "         0, 11,  1,  5, 11, 10, 10, 16,  7,  7,  7, 12,  2,  7, 13,  0, 10, 11,\n",
            "        10,  9,  4,  7, 16,  9,  9,  8, 15,  7,  1, 16,  3,  2, 14,  4, 14,  7,\n",
            "         1,  8, 11,  9,  9, 13,  2,  7, 15,  9])\n",
            "\n",
            "Batch 2:\n",
            "X shape: torch.Size([64, 3000, 3])\n",
            "y shape: torch.Size([64])\n",
            "y: tensor([ 3, 14, 15, 14,  1, 15,  8, 10,  1, 14, 10,  1,  8,  2, 11,  7,  8,  2,\n",
            "         9, 13, 17, 12,  2,  8, 11,  6,  2, 15,  7, 11, 10,  2, 10,  8,  9, 12,\n",
            "        12,  3, 15,  8,  3,  3, 12, 10,  7,  2, 12,  9,  6, 13,  7,  3,  4, 12,\n",
            "         1, 14, 12, 10,  5,  7, 13, 11, 10,  1])\n",
            "\n",
            "Batch 3:\n",
            "X shape: torch.Size([64, 3000, 3])\n",
            "y shape: torch.Size([64])\n",
            "y: tensor([ 7, 15, 10, 13, 13, 15, 16, 10,  5, 14, 14,  9,  7,  3,  9,  9, 14,  7,\n",
            "         7, 10, 12,  0, 13,  7,  3,  2,  5,  7, 11,  7,  0, 10, 12, 13,  2,  2,\n",
            "        12, 14,  9, 17,  9, 17,  6, 13,  0,  4, 15, 11,  7, 10, 11,  7, 13, 14,\n",
            "        17, 14,  2,  3, 17,  2,  7, 15, 15,  9])\n",
            "\n",
            "Batch 4:\n",
            "X shape: torch.Size([64, 3000, 3])\n",
            "y shape: torch.Size([64])\n",
            "y: tensor([ 7,  4,  4, 12,  7, 17, 13,  8, 10,  3, 10,  4, 10, 16, 12, 16, 13,  8,\n",
            "         5, 10,  0, 13, 14, 13,  1,  2, 11,  9,  7, 12, 13,  9,  6, 10, 13,  8,\n",
            "        13,  4,  9, 17,  1, 14, 13, 14,  9, 11, 17,  1,  0, 12, 10,  9, 17, 14,\n",
            "         9, 14,  8, 10,  8, 11, 11,  7, 11, 12])\n",
            "\n",
            "Batch 5:\n",
            "X shape: torch.Size([64, 3000, 3])\n",
            "y shape: torch.Size([64])\n",
            "y: tensor([ 1,  0,  7,  2,  9,  7, 16,  7,  9, 14,  2,  7,  9, 10,  7,  5, 13, 10,\n",
            "        10, 11, 13, 14,  9,  8, 12,  2,  6, 11,  0,  7, 15, 11, 13,  8, 15, 10,\n",
            "         7, 13,  1,  5, 16,  0,  9, 10,  7, 13, 13, 13,  3, 10,  7,  7,  1,  2,\n",
            "        17, 14, 11, 10, 14,  1,  3,  2,  9, 16])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####################################3"
      ],
      "metadata": {
        "id": "4wvu-rqJ3LUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMAutoencoder(L.LightningModule):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(LSTMAutoencoder, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        # Decoder\n",
        "        self.decoder = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        # Map the output of the decoder to the input space\n",
        "        self.output_layer = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "        self.TrainLossEs = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoding\n",
        "        _, (hidden, cell) = self.encoder(x)  # We only use the hidden states; ignore the outputs\n",
        "\n",
        "        # Prepare decoder initial input (typically zeros, but you can modify this)\n",
        "        decoder_input = torch.zeros(x.size(0), x.size(1), self.hidden_size).to(x.device)  # Size: (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # Decoding\n",
        "        decoded, _ = self.decoder(decoder_input, (hidden, cell))  # Pass hidden states as initial state\n",
        "\n",
        "        # Mapping decoded states to input space\n",
        "        decoded = self.output_layer(decoded)  # Size: (batch_size, seq_len, input_size)\n",
        "        return decoded\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "      sequences, _ = batch  # Assuming the batch returns a tuple (sequences, features)\n",
        "      sequences = sequences.float()  # Ensure sequences are of type float for LSTM\n",
        "\n",
        "      # Ensure sequences are correctly shaped [batch_size, seq_length, num_features]\n",
        "      reconstructed = self(sequences)\n",
        "\n",
        "      # Compute the loss\n",
        "      loss = F.mse_loss(reconstructed, sequences)\n",
        "      self.TrainLossEs.append(loss.item())\n",
        "      self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.0001)\n",
        "\n",
        "    def get_train_loss(self):\n",
        "        return self.TrainLossEs\n"
      ],
      "metadata": {
        "id": "Vl3Y4FlXBBB0"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#Pretrain the Autoencoder**"
      ],
      "metadata": {
        "id": "LJLd1N6eBSZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "\n",
        "\n",
        "# Define your autoencoder model\n",
        "autoencoder = LSTMAutoencoder(input_size=3, hidden_size=64, num_layers=5)\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    monitor='train_loss',\n",
        "    filename='Autoencoder-{epoch:02d}-{train_loss:.2f}',\n",
        "    save_top_k=1,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='train_loss',\n",
        "    patience=3,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "trainer = L.Trainer(\n",
        "      max_epochs=15,\n",
        "      accelerator='auto',\n",
        "      devices='auto',\n",
        "      callbacks=[model_checkpoint, early_stopping],\n",
        "      accumulate_grad_batches=2,\n",
        "      gradient_clip_val=0.5,  # Adjust gradient clipping as needed\n",
        "      logger=TensorBoardLogger('lightning_logs', name='Autoencoder'),\n",
        "      check_val_every_n_epoch=1,  # Run validation every epoch\n",
        "      log_every_n_steps=10,  # Adjust according to your preference\n",
        "  )\n",
        "\n",
        "# Pretrain the model\n",
        "trainer.fit(autoencoder, train_dataloader, val_dataloader)"
      ],
      "metadata": {
        "id": "1qGSFAH9y8gH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711,
          "referenced_widgets": [
            "5f442acd825c4f87878574dbbf99fd81",
            "3f18cfd51eb04d129da0b4788cd1548b",
            "153d7acbdf47492da14f2a251b69a5fa",
            "6c9df3cfa50942a2a2afb71b11d45ef1",
            "8406bb43a1674e5aa1bd8227fdb32de1",
            "2a973e6dfdde4bb8a330f02ebe2f697a",
            "a7e36c02f9a74adb916ec4f933183798",
            "9c09ae922f764dabafc6086bce653114",
            "d7150af05dcd445a972a284c0848fc99",
            "514b0f85b32c475d809a06c68be6271e",
            "792c37f27af64e688d6e1ab57a904a84"
          ]
        },
        "outputId": "c0ef57dc-0980-4370-9253-791fd41573b7"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
            "WARNING: Missing logger folder: lightning_logs/Autoencoder\n",
            "WARNING:lightning.pytorch.loggers.tensorboard:Missing logger folder: lightning_logs/Autoencoder\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name         | Type   | Params\n",
            "----------------------------------------\n",
            "0 | encoder      | LSTM   | 150 K \n",
            "1 | decoder      | LSTM   | 166 K \n",
            "2 | output_layer | Linear | 195   \n",
            "----------------------------------------\n",
            "317 K     Trainable params\n",
            "0         Non-trainable params\n",
            "317 K     Total params\n",
            "1.270     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type   | Params\n",
            "----------------------------------------\n",
            "0 | encoder      | LSTM   | 150 K \n",
            "1 | decoder      | LSTM   | 166 K \n",
            "2 | output_layer | Linear | 195   \n",
            "----------------------------------------\n",
            "317 K     Trainable params\n",
            "0         Non-trainable params\n",
            "317 K     Total params\n",
            "1.270     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f442acd825c4f87878574dbbf99fd81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: `Trainer.fit` stopped: `max_epochs=15` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=15` reached.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from google.colab import files\n",
        "\n",
        "# Define the filename\n",
        "model_file_name = 'autoencoder_final.pth'\n",
        "\n",
        "# Save the autoencoder model's state dictionary\n",
        "torch.save(autoencoder.state_dict(), model_file_name)"
      ],
      "metadata": {
        "id": "Fl3Z7aehVtB4"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file to your local machine\n",
        "files.download(model_file_name)"
      ],
      "metadata": {
        "id": "52YT1wh3VyrL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "189f6ae2-3839-4df2-d5dd-887445fb5ec4"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_83073193-6f7c-435c-8f69-da3f2fcd30fd\", \"autoencoder_final.pth\", 1282596)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path for saving model's state dictionary\n",
        "model_state_dict_path = 'autoencoder_state_dict.pth'\n",
        "\n",
        "# Save the autoencoder model's state dictionary\n",
        "torch.save(autoencoder.state_dict(), model_state_dict_path)\n"
      ],
      "metadata": {
        "id": "uHUerHYYyodo"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model structure\n",
        "autoencoder_model = LSTMAutoencoder(input_size=3, hidden_size=64, num_layers=5)\n",
        "\n",
        "# Load the state dict back into the model\n",
        "autoencoder_model.load_state_dict(torch.load(model_state_dict_path))\n"
      ],
      "metadata": {
        "id": "2XJMMsWfyqxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec60e271-3b66-41d3-cf57-febf5f7a0b78"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#Define and Train the Classification Model**"
      ],
      "metadata": {
        "id": "zwWUVaPiBZlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class LSTMClassifier(L.LightningModule):\n",
        "#     def _init_(self, input_size, dropout_rate):\n",
        "#         super(LSTMClassifier, self)._init_()\n",
        "#         self.input_size = input_size\n",
        "#         self.dropout_rate = dropout_rate\n",
        "\n",
        "#         # Use the pretrained LSTMAutoencoder's encoder\n",
        "#         self.encoder = autoencoder.encoder  # Use only the encoder part\n",
        "\n",
        "#         # Assuming the hidden_size and num_layers match those of the pretrained encoder\n",
        "#         hidden_size = autoencoder.hidden_size\n",
        "\n",
        "#         # Additional linear layer for classification\n",
        "#         self.fc = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "#         # Optionally add a dropout layer\n",
        "#         self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "#         # save all the metrics\n",
        "#         self.TrainLossEs = []\n",
        "#         self.ValLossEs = []\n",
        "#         self.TrainAcc = []\n",
        "#         self.ValAcc = []\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Use the encoder to get the hidden state\n",
        "#         (hidden) = self.encoder(x)  # Get the last hidden state\n",
        "\n",
        "#         # Applying dropout\n",
        "#         hidden = self.dropout(hidden[-1])  # Taking the last layer's hidden state\n",
        "\n",
        "#         # Pass through the classification layer\n",
        "#         out = self.fc(hidden)\n",
        "#         return out\n",
        "\n",
        "\n",
        "# # class LSTMClassifier(L.LightningModule):\n",
        "# #     def __init__(self, pretrained_encoder, num_classes, dropout_rate=0.4):\n",
        "# #         super(LSTMClassifier, self).__init__()\n",
        "# #         self.encoder = pretrained_encoder  # Use the pretrained encoder\n",
        "# #         self.fc = nn.Linear(pretrained_encoder.hidden_size, num_classes)  # Classification head\n",
        "# #         self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "\n",
        "#     # def forward(self, x):\n",
        "#     #   # Pass input through the encoder\n",
        "#     #   (hidden) = self.encoder(x)\n",
        "#     #   x = hidden[-1]  # Take the last hidden state\n",
        "#     #   # Pass through the classifier\n",
        "#     #   x = self.dropout(x)\n",
        "#     #   x = self.fc(x)\n",
        "#     #   return x\n",
        "\n",
        "\n",
        "#     def training_step(self, batch, batch_idx):\n",
        "#         sequences, labels = batch\n",
        "#         predictions = self(sequences)\n",
        "#         loss = F.cross_entropy(predictions, labels)\n",
        "#         acc = (predictions.argmax(dim=1) == labels).float().mean()\n",
        "#         self.TrainLossEs.append(loss.item())\n",
        "#         self.TrainAcc.append(acc.item())\n",
        "#         self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "#         self.log('train_accuracy', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
        "#         return loss\n",
        "\n",
        "#     def validation_step(self, batch, batch_idx):\n",
        "#         sequences, labels = batch\n",
        "#         predictions = self(sequences)\n",
        "#         loss = F.cross_entropy(predictions, labels)\n",
        "#         acc = (predictions.argmax(dim=1) == labels).float().mean()\n",
        "#         self.ValLossEs.append(loss.item())\n",
        "#         self.ValAcc.append(acc.item())\n",
        "#         self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "#         self.log('val_accuracy', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "#     def configure_optimizers(self):\n",
        "#         optimizer = torch.optim.Adam(self.parameters(), lr=0.0005)\n",
        "#         scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "#         return [optimizer], [scheduler]\n",
        "\n",
        "#     def get_train_loss(self):\n",
        "#         return self.TrainLossEs\n",
        "\n",
        "#     def get_val_loss(self):\n",
        "#         return self.ValLossEs\n",
        "\n",
        "#     def get_train_acc(self):\n",
        "#         return self.TrainAcc\n",
        "\n",
        "#     def get_val_acc(self):\n",
        "#         return self.ValAcc"
      ],
      "metadata": {
        "id": "UBKeSgxUBZEo"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(L.LightningModule):\n",
        "    def __init__(self, input_size=3, hidden_size=64, num_layers=5, num_classes=18, dropout_rate=0.4):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Assuming 'autoencoder_model' is a pretrained instance of your LSTMAutoencoder class\n",
        "        self.encoder = autoencoder.encoder  # Make sure this is defined or passed correctly\n",
        "\n",
        "        # Classifier layers\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)  # Maps from hidden state space to label space\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Metrics storage\n",
        "        self.TrainLosses = []\n",
        "        self.ValLosses = []\n",
        "        self.TrainAcc = []\n",
        "        self.ValAcc = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        _, (hidden, _) = self.encoder(x)  # We only need the hidden state\n",
        "\n",
        "        # We take the last hidden state for the last layer of the LSTM\n",
        "        # The hidden state shape is (num_layers, batch_size, hidden_size), we take the last layer's output\n",
        "        last_hidden = hidden[-1]  # Shape: (batch_size, hidden_size)\n",
        "\n",
        "        # Dropout and classification\n",
        "        last_hidden = self.dropout(last_hidden)  # Apply dropout\n",
        "        out = self.fc(last_hidden)  # Final classification\n",
        "        return out\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        self.train()\n",
        "        sequences, labels = batch  # Unpack batch\n",
        "        predictions = self(sequences)  # Forward pass\n",
        "        loss = F.cross_entropy(predictions, labels)  # Loss calculation\n",
        "        acc = (predictions.argmax(dim=1) == labels).float().mean()  # Accuracy calculation\n",
        "\n",
        "        # Logging\n",
        "        self.TrainLosses.append(loss.item())\n",
        "        self.TrainAcc.append(acc.item())\n",
        "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        self.log('train_accuracy', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        sequences, labels = batch\n",
        "        predictions = self(sequences)\n",
        "        loss = F.cross_entropy(predictions, labels)\n",
        "        # Calculate accuracy\n",
        "        acc = (predictions.argmax(dim=1) == labels).float().mean()\n",
        "        self.ValLossEs.append(loss.item())\n",
        "        self.ValAcc.append(acc.item())\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('val_accuracy', acc, prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0005)\n",
        "        return optimizer\n",
        "\n",
        "    def get_train_loss(self):\n",
        "        return self.TrainLossEs\n",
        "\n",
        "    def get_val_loss(self):\n",
        "        return self.ValLossEs\n",
        "\n",
        "    def get_train_acc(self):\n",
        "        return self.TrainAcc\n",
        "\n",
        "    def get_val_acc(self):\n",
        "        return self.ValAcc"
      ],
      "metadata": {
        "id": "WqEYUqcZXZdD"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "7gTD2ktAJbo1"
      },
      "outputs": [],
      "source": [
        "# Create dataset instances\n",
        "train_dataset = HARDataset(train_features_loaded, unlabeled_dir, labeled=True)\n",
        "val_dataset = HARDataset(val_features_loaded, unlabeled_dir, labeled=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "m7PaQKlFJbo2"
      },
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=pad_sequences_from_start)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=pad_sequences_from_start)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (x, y) in enumerate(train_dataloader):\n",
        "    if i == 5:\n",
        "        break\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    print(f\"X shape: {x.shape}\")\n",
        "    print(f\"y shape: {y.shape}\")\n",
        "    print(f\"y: {y}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "B3i-NWscJbo2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a511b3ec-4376-400a-8dea-fedc0d3c9bb5"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1:\n",
            "X shape: torch.Size([64, 3000, 3])\n",
            "y shape: torch.Size([64])\n",
            "y: tensor([ 8, 11, 14, 12, 12,  1, 11,  0, 10,  5, 12, 13, 11,  7,  3, 15, 15,  8,\n",
            "         7,  9, 12,  9, 12,  2, 12, 11, 13,  5, 12, 10, 10,  7, 15, 10,  2, 17,\n",
            "         8,  9, 15,  9, 11,  9, 13,  2,  4, 12, 16, 15,  1, 14,  2,  1,  7,  2,\n",
            "        13,  3,  1, 10,  6, 17, 10,  8,  2,  1])\n",
            "\n",
            "Batch 2:\n",
            "X shape: torch.Size([64, 3000, 3])\n",
            "y shape: torch.Size([64])\n",
            "y: tensor([17,  5,  5,  3, 10, 13, 10,  9,  0, 14,  9, 10, 15,  7, 14,  4,  6,  9,\n",
            "        10,  7,  5,  1,  3, 11, 14,  8, 10,  2, 15, 15,  9,  8,  8,  3,  9,  2,\n",
            "         8, 13, 10,  9, 10,  5,  6, 10, 10,  2,  1, 12, 11,  9,  9,  3, 11,  9,\n",
            "        10,  5,  9, 15, 16, 10,  7,  6, 15, 15])\n",
            "\n",
            "Batch 3:\n",
            "X shape: torch.Size([64, 3000, 3])\n",
            "y shape: torch.Size([64])\n",
            "y: tensor([13, 10,  2,  9, 13,  7,  9,  6, 12, 11, 11, 11,  5, 13,  3,  7, 12, 13,\n",
            "         8, 12, 12,  1,  6, 10,  4, 12,  4,  9, 10, 11, 14,  3,  4,  2,  9, 12,\n",
            "        14,  5, 11, 13,  4,  3, 10,  2,  2, 13,  8, 16, 13,  9,  2,  9, 16, 12,\n",
            "        14,  4,  4,  1, 13, 13, 14,  8,  0, 12])\n",
            "\n",
            "Batch 4:\n",
            "X shape: torch.Size([64, 3000, 3])\n",
            "y shape: torch.Size([64])\n",
            "y: tensor([16,  8, 10, 14,  4,  9,  7,  3,  2, 10, 14, 12,  5, 10,  5, 15, 12,  1,\n",
            "         1,  5, 15,  7,  6, 15, 13,  7,  3, 13, 12, 11,  8, 12,  9,  8, 11,  5,\n",
            "        10, 10,  3, 15,  9, 12,  1, 14, 10, 13,  7, 16,  1, 13, 12,  0, 15, 12,\n",
            "         5,  9,  7, 16,  2, 15,  5, 13, 12,  3])\n",
            "\n",
            "Batch 5:\n",
            "X shape: torch.Size([64, 3000, 3])\n",
            "y shape: torch.Size([64])\n",
            "y: tensor([13, 12, 10,  9, 12,  6,  9, 17,  2,  0,  3, 15,  9,  7, 16,  0,  8, 11,\n",
            "        11, 16,  9,  9, 11,  7, 16, 17,  6,  7,  6, 16, 10,  7,  4, 16, 12,  1,\n",
            "         1,  1, 10, 13,  3, 12,  6, 12, 10,  3,  9,  4,  2, 11, 10, 10,  3,  1,\n",
            "        13,  3, 11, 11, 11,  2, 11,  5, 10,  6])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "7ruovyL3oqFf"
      },
      "outputs": [],
      "source": [
        "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "\n",
        "def train_and_evaluate(model, train_dataloader, val_dataloader, max_epochs=10, patience=5):\n",
        "\n",
        "    # Logging\n",
        "    logger = TensorBoardLogger(\"lightning_logs\", name=\"LSTMClassifier\")\n",
        "\n",
        "    # Callbacks\n",
        "    model_checkpoint = ModelCheckpoint(\n",
        "        monitor='val_loss',  # Save the model based on the validation loss\n",
        "        filename='LSTMClassifier-{epoch:02d}-{val_loss:.2f}',\n",
        "        save_top_k=1,  # Save the best model only\n",
        "        mode='min',  # Minimize validation loss\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=5,  # Number of epochs with no improvement after which training will be stopped\n",
        "        mode='max',\n",
        "    )\n",
        "\n",
        "    # Trainer\n",
        "    trainer = L.Trainer(\n",
        "        default_root_dir='checkpoints/',\n",
        "        log_every_n_steps=5,\n",
        "        max_epochs=max_epochs,\n",
        "        accelerator=\"auto\",\n",
        "        devices=\"auto\",\n",
        "        callbacks=[model_checkpoint, early_stopping],\n",
        "        logger=logger\n",
        "    )\n",
        "\n",
        "    # Assuming 'trainer' is already defined and initialized\n",
        "    if hasattr(trainer, 'logger') and trainer.logger:\n",
        "        trainer.logger._log_graph = True  # This will log the model graph for visualization in TensorBoard\n",
        "        trainer.logger._default_hp_metric = None  # This will disable the default hyperparameter metric logging\n",
        "\n",
        "    # Start training\n",
        "    trainer.fit(model, train_dataloader, val_dataloader)\n",
        "\n",
        "    # Retrieve metrics from the trainer's logged metrics\n",
        "    train_loss = trainer.logged_metrics.get('train_loss')\n",
        "    val_loss = trainer.logged_metrics.get('val_loss')\n",
        "    train_acc = trainer.logged_metrics.get('train_accuracy')\n",
        "    val_acc = trainer.logged_metrics.get('val_accuracy')\n",
        "\n",
        "    metrics = {\n",
        "    'train_loss': train_loss,\n",
        "    'val_loss': val_loss,\n",
        "    'train_acc': train_acc,\n",
        "    'val_acc': val_acc\n",
        "    }\n",
        "\n",
        "    return model, metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_array(matric_batch_array, num_epoch):\n",
        "  \"\"\"\n",
        "  The  function calculates the mean matric value fo each epoch\n",
        "  \"\"\"\n",
        "  res = []\n",
        "  for i in np.array_split(matric_batch_array,num_epoch):\n",
        "    res.append(float(np.mean(i)))\n",
        "  return res"
      ],
      "metadata": {
        "id": "NuG4IKBKbAMj"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_acc_and_loss(model, num_epoch):\n",
        "  \"\"\"\n",
        "  Helper function to plot the graphs\n",
        "  \"\"\"\n",
        "\n",
        "  train_losses = epoch_array(model.get_train_loss(), num_epoch)\n",
        "  val_losses = epoch_array(model.get_val_loss(), num_epoch)\n",
        "  train_accs = epoch_array(model.get_train_acc(), num_epoch)\n",
        "  val_accs = epoch_array(model.get_val_acc(), num_epoch)\n",
        "\n",
        "  epochs = range(1, num_epoch + 1)\n",
        "\n",
        "  # Plot training and validation loss\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.plot(epochs, train_losses, '.-', label='Training Loss', color = '#609d9e')\n",
        "  plt.plot(epochs, val_losses, '.-', label='Validation Loss', color = '#075052')\n",
        "  plt.title('Training and Validation Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # Plot training and validation accuracy\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.plot(epochs, train_accs, '.-', label='Training Accuracy', color = '#609d9e')\n",
        "  plt.plot(epochs, val_accs, '.-', label='Validation Accuracy', color = '#075052')\n",
        "  plt.title('Training and Validation Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "nCkXrf4Hq45P"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize your classifier\n",
        "classifier = LSTMClassifier(input_size=3, hidden_size=64, num_layers=5, num_classes=18, dropout_rate=0.4)\n",
        "\n",
        "# Train your classifier (make sure to define 'train_and_evaluate' properly or use your training routine)\n",
        "classifier, metrics = train_and_evaluate(model=classifier, train_dataloader=train_dataloader, val_dataloader=val_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f0bd95cf0b9b48b0ae62f10e869e8ae3",
            "2f571408d2d74ccb9a9462fe54d16d70",
            "9624bcf440d04ce2b0654596310e282a",
            "93570097bf204baabee312c04eb1b1ca",
            "7a8e2d90e3fa4e8eb4d7e8ffa0e5f58c",
            "b52a28f8fc2847dabf4ee8b44541224b",
            "bd7b56a1d4cd4d36bf7a4fd81bbb8da5",
            "b434e43a0d394a9f85f03070b10cfc84",
            "6a7f1d65b3b14dec92b9d9abdc3bf12f",
            "9e088143188046f8b8b3800ee43d2dc0",
            "80ec9735e8c04ba2a989e2d249b78d2c"
          ]
        },
        "id": "nZpDMrqaXvpc",
        "outputId": "9ee5376a-56ab-4366-ef6e-d94ac74ccaf5"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name    | Type    | Params\n",
            "------------------------------------\n",
            "0 | encoder | LSTM    | 150 K \n",
            "1 | fc      | Linear  | 1.2 K \n",
            "2 | dropout | Dropout | 0     \n",
            "------------------------------------\n",
            "151 K     Trainable params\n",
            "0         Non-trainable params\n",
            "151 K     Total params\n",
            "0.608     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name    | Type    | Params\n",
            "------------------------------------\n",
            "0 | encoder | LSTM    | 150 K \n",
            "1 | fc      | Linear  | 1.2 K \n",
            "2 | dropout | Dropout | 0     \n",
            "------------------------------------\n",
            "151 K     Trainable params\n",
            "0         Non-trainable params\n",
            "151 K     Total params\n",
            "0.608     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loggers/tensorboard.py:193: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0bd95cf0b9b48b0ae62f10e869e8ae3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Early stopping conditioned on metric `val_accuracy` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `train_loss`, `train_accuracy`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-217-076e90b9e115>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train your classifier (make sure to define 'train_and_evaluate' properly or use your training routine)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-214-ee56685a273c>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_dataloader, val_dataloader, max_epochs, patience)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Retrieve metrics from the trainer's logged metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         )\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected state {self.state}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\u001b[0m in \u001b[0;36mon_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_train_epoch_end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitoring_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_lightning_module_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_train_epoch_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_train_epoch_end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitoring_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Callback]{callback.state_key}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/early_stopping.py\u001b[0m in \u001b[0;36mon_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_on_train_epoch_end\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_skip_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_early_stopping_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/early_stopping.py\u001b[0m in \u001b[0;36m_run_early_stopping_check\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         if trainer.fast_dev_run or not self._validate_condition_metric(  # disable early_stopping with fast_dev_run\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0mlogs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         ):  # short circuit if metric not present\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/early_stopping.py\u001b[0m in \u001b[0;36m_validate_condition_metric\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmonitor_val\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mrank_zero_warn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRuntimeWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Early stopping conditioned on metric `val_accuracy` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `train_loss`, `train_accuracy`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "plot_acc_and_loss(classifier, epochs)"
      ],
      "metadata": {
        "id": "4PIjt8GWJH5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_predictions(model, dataloader):\n",
        "    model.eval()\n",
        "    model.freeze()\n",
        "    predictions, labels, probabilities = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels_batch in dataloader:\n",
        "            outputs = model(sequences)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            labels.extend(labels_batch.cpu().numpy())\n",
        "            probabilities.extend(probs.cpu().numpy())\n",
        "    return predictions, labels, probabilities\n",
        "\n",
        "# Extract predictions and probabilities from the validation set\n",
        "predictions, true_labels, probabilities = extract_predictions(classifier, val_dataloader)"
      ],
      "metadata": {
        "id": "Qyx7aObsurvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_classifications_df(predictions, true_labels, probabilities, label_encoder, num_examples=5):\n",
        "    # Convert numerical labels back to original activity names\n",
        "    predicted_activities = label_encoder.inverse_transform(predictions)\n",
        "    true_activities = label_encoder.inverse_transform(true_labels)\n",
        "\n",
        "    # Prepare data for DataFrame\n",
        "    data = []\n",
        "    good_indices = [i for i, (pred, true) in enumerate(zip(predictions, true_labels)) if pred == true][:num_examples]\n",
        "    for i in good_indices:\n",
        "        data.append({\n",
        "            'Index': i,\n",
        "            'Type': 'Good',\n",
        "            'Predicted': predicted_activities[i],\n",
        "            'True': true_activities[i],\n",
        "            'Probability': max(probabilities[i])\n",
        "        })\n",
        "\n",
        "    bad_indices = [i for i, (pred, true) in enumerate(zip(predictions, true_labels)) if pred != true][:num_examples]\n",
        "    for i in bad_indices:\n",
        "        data.append({\n",
        "            'Index': i,\n",
        "            'Type': 'Bad',\n",
        "            'Predicted': predicted_activities[i],\n",
        "            'True': true_activities[i],\n",
        "            'Probability': max(probabilities[i])\n",
        "        })\n",
        "\n",
        "    uncertain_indices = sorted(range(len(probabilities)), key=lambda i: max(probabilities[i]))[:num_examples]\n",
        "    for i in uncertain_indices:\n",
        "        data.append({\n",
        "            'Index': i,\n",
        "            'Type': 'Uncertain',\n",
        "            'Predicted': predicted_activities[i],\n",
        "            'True': true_activities[i],\n",
        "            'Probability': max(probabilities[i])\n",
        "        })\n",
        "\n",
        "    # Create and return DataFrame\n",
        "    results_df = pd.DataFrame(data)\n",
        "\n",
        "    # Round the 'Probability' column to 4 decimal places\n",
        "    results_df['Probability'] = results_df['Probability'].round(4)\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "wlj3bBNnu7D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_classifications_df(predictions, true_labels, probabilities, label_encoder, num_examples=5)"
      ],
      "metadata": {
        "id": "l3-DvSAVR-ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xNh-C4jY6b16"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}